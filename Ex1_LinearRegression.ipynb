{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac71e7f-c88d-414a-be35-61daaa4c543f",
   "metadata": {},
   "source": [
    "# Example 1: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c6e99-ee91-4b00-9a9d-cfa90d3209c4",
   "metadata": {},
   "source": [
    "Suppose you have the following dataset (taken from Module 2). \n",
    "\n",
    "| $X_1$  | $X_2$  | $X_3$  | $y$  |\n",
    "|--------|--------|--------|--------|\n",
    "| 1.2 | 2.3 | 3.1 | 8.2 |\n",
    "| 2.0 | 1.8 | 2.6 | 7.5 |\n",
    "| 3.1 | 2.7 | 4.0 | 10.3 |\n",
    "| 4.0 | 3.9 | 5.1 | 13.2 |\n",
    "| 5.2 | 4.7 | 6.0 | 15.1 |\n",
    "| 6.0 | 5.9 | 7.2 | 17.2 |\n",
    "| 7.1 | 6.8 | 8.1 | 19.0 |\n",
    "\n",
    "We define \n",
    "\n",
    "$$\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "1 & 1.2 & 2.3 & 3.1 \\\\\n",
    "1 & 2.0 & 1.8 & 2.6 \\\\\n",
    "1 & 3.1 & 2.7 & 4.0 \\\\\n",
    "1 & 4.0 & 3.9 & 5.1 \\\\\n",
    "1 & 5.2 & 4.7 & 6.0 \\\\\n",
    "1 & 6.0 & 5.9 & 7.2 \\\\\n",
    "1 & 7.1 & 6.8 & 8.1 \\\\\n",
    "\\end{bmatrix} \\quad \\text{ and } \\quad\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "8.2 \\\\\n",
    "7.5 \\\\\n",
    "10.3 \\\\\n",
    "13.2 \\\\\n",
    "15.1 \\\\\n",
    "17.2 \\\\\n",
    "19.0 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\\mathbf{X}^T =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1\\\\\n",
    "1.2 & 2.0 & 3.1 & 4.0 & 5.2 & 6.0 & 7.1 \\\\\n",
    "2.3 & 1.8 & 2.7 & 3.9 & 4.7 & 5.9 & 6.8 \\\\\n",
    "3.1 & 2.6 & 4.0 & 5.1 & 6.0 & 7.2 & 8.1 \\\\\n",
    "\\end{bmatrix}, \\quad \\quad \n",
    "\\mathbf{X}^T \\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "7 & 28.6 & 28.1 & 36.1 \\\\\n",
    "28.6 & 144.5 & 138.45 & 173.63 \\\\\n",
    "28.1 & 138.45 & 134.17 & 168.26 \\\\\n",
    "36.1 & 173.63 & 168.26 & 211.83 \\\\\n",
    "\\end{bmatrix}, \\quad \\text{and}$$ $$ \\mathbf{X}^T \\mathbf{y} =\n",
    "\\begin{bmatrix}\n",
    "90.5 \\\\\n",
    "426.19 \\\\\n",
    "413.3 \\\\\n",
    "521.78 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The inverse of $(\\mathbf{X}^T \\mathbf{X})^{-1}$ is:\n",
    "\n",
    "$$(\\mathbf{X}^T \\mathbf{X})^{-1} =\n",
    "\\begin{bmatrix}\n",
    "8.8541859 & 2.16305293 & 7.59490793 & -9.31466836 \\\\\n",
    "2.16305293 & 1.14062489 & 1.22606673 & -2.2774437 \\\\\n",
    "7.59490793 & 1.22606673 & 9.09088261 & -9.52032314 \\\\\n",
    "-9.31466836 & -2.2774437 & -9.52032314 & 11.0210152 \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We now have all the elements to find $\\mathbf{\\hat{\\beta}}$:\n",
    "\n",
    "\\begin{align*} \n",
    "\\mathbf{\\hat{\\beta}} &= (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "8.8541859 & 2.16305293 & 7.59490793 & -9.31466836 \\\\\n",
    "2.16305293 & 1.14062489 & 1.22606673 & -2.2774437 \\\\\n",
    "7.59490793 & 1.22606673 & 9.09088261 & -9.52032314 \\\\\n",
    "-9.31466836 & -2.2774437 & -9.52032314 & 11.0210152 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "90.5 \\\\\n",
    "426.19 \\\\\n",
    "413.3 \\\\\n",
    "521.78 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "1.94314074 \\\\\n",
    "0.28801913 \\\\\n",
    "-0.37587751 \\\\\n",
    "2.19453811 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "This means our model is of the form \n",
    "\n",
    "$$ \\hat{Y} = 1.94314074 + 0.28801913 X_1 + -0.37587751 X_2 + 2.19453811  X_3 $$\n",
    "\n",
    "Pay careful attention to the order of the values of $\\beta$ and the order of the columns in $\\mathbf{x}$; these should correspond with one another!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3388c3d-6d7b-4faf-b987-684e93d19d72",
   "metadata": {},
   "source": [
    "## Explicit Computation in Code \n",
    "\n",
    "We can compute this by hand, with a graphing calculator or explicitly by code. Below is the code for this explicit computation in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124a3d6d-e0d6-4441-8c6e-fde233ce1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy is a library for arrays in python\n",
    "# and it is often abbreviated to \"np\"\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d78ba5-69d0-4543-91ff-f1ea0ac07f06",
   "metadata": {},
   "source": [
    "This is the dataset from the problem. We encode it into a numpy array which tends to be more efficient than a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb86371-eb01-4d0c-91d2-9a4f9292d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix x\n",
    "X = np.array([[1, 1.2, 2.3, 3.1],\n",
    "              [1, 2.0, 1.8, 2.6],\n",
    "              [1, 3.1, 2.7, 4.0],\n",
    "              [1, 4.0, 3.9, 5.1],\n",
    "              [1, 5.2, 4.7, 6.0],\n",
    "              [1, 6.0, 5.9, 7.2],\n",
    "              [1, 7.1, 6.8, 8.1]])\n",
    "\n",
    "# Create matrix y\n",
    "y = np.array([[8.2],\n",
    "              [7.5],\n",
    "              [10.3],\n",
    "              [13.2],\n",
    "              [15.1],\n",
    "              [17.2],\n",
    "              [19.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3005839f-b78f-4926-88cf-8e8cab30acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate x^T\n",
    "X_t = np.transpose(X)\n",
    "\n",
    "# Calculate the product of x^T and x\n",
    "prod_X_t_X = np.dot(X_t, X)\n",
    "\n",
    "# Calculate the inverse of the product of X^T and X\n",
    "inv_prod_X_t_X = np.linalg.inv(prod_X_t_X)\n",
    "\n",
    "# Calculate the product of X^T and y\n",
    "prod_X_t_y = np.dot(X_t, y)\n",
    "\n",
    "# Calculate coefficients\n",
    "beta = np.dot(inv_prod_X_t_X, prod_X_t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab312a9-674d-4cac-b723-5cadeac478a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X^T:\n",
      "[[1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.2 2.  3.1 4.  5.2 6.  7.1]\n",
      " [2.3 1.8 2.7 3.9 4.7 5.9 6.8]\n",
      " [3.1 2.6 4.  5.1 6.  7.2 8.1]]\n",
      "\n",
      "Product of X^T and X:\n",
      "[[  7.    28.6   28.1   36.1 ]\n",
      " [ 28.6  144.5  138.45 173.63]\n",
      " [ 28.1  138.45 134.17 168.26]\n",
      " [ 36.1  173.63 168.26 211.83]]\n",
      "\n",
      "Inverse of the product of X^T and X, (X^T X)^{-1}:\n",
      "[[ 8.8541859   2.16305293  7.59490793 -9.31466836]\n",
      " [ 2.16305293  1.14062489  1.22606673 -2.2774437 ]\n",
      " [ 7.59490793  1.22606673  9.09088261 -9.52032314]\n",
      " [-9.31466836 -2.2774437  -9.52032314 11.0210152 ]]\n",
      "\n",
      "Product of X^T and y:\n",
      "[[ 90.5 ]\n",
      " [426.19]\n",
      " [413.3 ]\n",
      " [521.78]]\n",
      "\n",
      "Coefficients for Linear Regression:\n",
      "[[ 1.94314074]\n",
      " [ 0.28801913]\n",
      " [-0.37587751]\n",
      " [ 2.19453811]]\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"X^T:\")\n",
    "print(X_t)\n",
    "print(\"\\nProduct of X^T and X:\")\n",
    "print(prod_X_t_X)\n",
    "print(\"\\nInverse of the product of X^T and X, (X^T X)^{-1}:\")\n",
    "print(inv_prod_X_t_X)\n",
    "print(\"\\nProduct of X^T and y:\")\n",
    "print(prod_X_t_y)\n",
    "print(\"\\nCoefficients for Linear Regression:\")\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1eb8d-1ba1-4d39-b0a9-63ae5d54d7e4",
   "metadata": {},
   "source": [
    "## Coding Linear Regression Quickly\n",
    "\n",
    "There are several libraries that support the computation of linear regression. Below, we will demonstrate two that we will use often:\n",
    "1. statsmodels\n",
    "2. scikit-learn\n",
    "\n",
    "Both packages are commonly used and have different strengths. Statsmodels is specifically designed from a statistics perspective. Scikit-Learn is a standardized model-building software that is ubiquitous in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167230fc-fa31-47f1-9822-b196c0bb6df8",
   "metadata": {},
   "source": [
    "### Statsmodels approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebee0f3-9cb5-4ceb-bf91-9374d1f5957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b841085-f0e8-41ae-8487-10b3a66da8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Note: we are using the various defined the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e095e9-5b77-4bc9-9a8a-b553368897f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.94314074  0.28801913 -0.37587751  2.19453811]\n"
     ]
    }
   ],
   "source": [
    "print(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd32a2-fd82-4ad9-974d-a88e0541a758",
   "metadata": {},
   "source": [
    "Above are the coefficients of the model corresponding to the function \n",
    "$$\\hat y = 1.94314074 + 0.28801913 X_1 - 0.37587751 X_2 + 2.19453811 X_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7289c9-11af-404d-ac9f-1e4896d41cbd",
   "metadata": {},
   "source": [
    "## Scikit Learn approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe3ba12-45e9-4538-b2e1-c675987c0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65cf47d6-1f20-43ad-a64a-a7c34db93c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a LinearRegression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec70b0af-d924-4691-9c37-458c2f66aaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [1.94314074]\n",
      "Coefficients: [[ 0.          0.28801913 -0.37587751  2.19453811]]\n"
     ]
    }
   ],
   "source": [
    "# Print the intercept and coefficients\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d02a4ab-b8cb-4128-ac50-fe6f7de16217",
   "metadata": {},
   "source": [
    "Scikit-learn separates out intercepts from coefficients and automatically includes the column we need to compute the intercept for the model. Because our $X$ is defined with this extra column of 1s, the corresponding coefficient is 0 and the intercept is separate. If we coded this more in-line with how scikit-learn is standardized, we would do this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95e38114-6fb1-427f-9829-87e5dd070dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [1.94314074]\n",
      "Coefficients: [[ 0.28801913 -0.37587751  2.19453811]]\n"
     ]
    }
   ],
   "source": [
    "# Create matrix x\n",
    "X = np.array([[1.2, 2.3, 3.1],\n",
    "              [2.0, 1.8, 2.6],\n",
    "              [3.1, 2.7, 4.0],\n",
    "              [4.0, 3.9, 5.1],\n",
    "              [5.2, 4.7, 6.0],\n",
    "              [6.0, 5.9, 7.2],\n",
    "              [7.1, 6.8, 8.1]])\n",
    "\n",
    "# Create matrix y\n",
    "y = np.array([[8.2],\n",
    "              [7.5],\n",
    "              [10.3],\n",
    "              [13.2],\n",
    "              [15.1],\n",
    "              [17.2],\n",
    "              [19.0]])\n",
    "\n",
    "# Create a LinearRegression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print the intercept and coefficients\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e3efc-164c-4c95-b6f7-d10e45f00615",
   "metadata": {},
   "source": [
    "These coefficients and intercept corresponding to the function \n",
    "$$\\hat y = 1.94314074 + 0.28801913 X_1 - 0.37587751 X_2 + 2.19453811 X_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa2b48-70fe-4d98-aad1-9520cb848e36",
   "metadata": {},
   "source": [
    "## Statistical Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25633f31-a840-4b3d-b772-f8bc1e6019c8",
   "metadata": {},
   "source": [
    "When we wish to perform statistical tests, this turns out to be an easy exercise with the statsmodels package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f20903f-c496-46c8-bf90-454754b96352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac5035-f4b9-40d4-a3d2-87defeb384f6",
   "metadata": {},
   "source": [
    "For this example, we will create a dataset using random number generation. Our true model is $$y = 2 + 0.3X.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2d01de4-5842-427c-9dfe-f54686961ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "np.random.seed(1)\n",
    "X = 2.5 * np.random.randn(100) + 3      # Independent variable\n",
    "epsilon = 0.5 * np.random.randn(100)    # Irreducible error\n",
    "y = 2 + 0.3 * X + epsilon               # Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e55de-b16d-4dce-98e3-993835b8556a",
   "metadata": {},
   "source": [
    "Within the code below, we compute the linear regression model. Notice that since we are not manually constructing the matrix $X$, we need to specify that a coefficient must be added (this is mathematically equivalent to adding a column of 1s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05fe9834-425d-43a1-95c2-7fc9bc252966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.698\n",
      "Model:                            OLS   Adj. R-squared:                  0.695\n",
      "Method:                 Least Squares   F-statistic:                     226.9\n",
      "Date:                Sat, 25 Nov 2023   Prob (F-statistic):           3.00e-27\n",
      "Time:                        17:00:49   Log-Likelihood:                -65.124\n",
      "No. Observations:                 100   AIC:                             134.2\n",
      "Df Residuals:                      98   BIC:                             139.5\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.0163      0.082     24.716      0.000       1.854       2.178\n",
      "x1             0.3191      0.021     15.062      0.000       0.277       0.361\n",
      "==============================================================================\n",
      "Omnibus:                        0.898   Durbin-Watson:                   2.157\n",
      "Prob(Omnibus):                  0.638   Jarque-Bera (JB):                0.561\n",
      "Skew:                          -0.172   Prob(JB):                        0.755\n",
      "Kurtosis:                       3.127   Cond. No.                         7.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#-------------- Compute Linear Regression Stats --------------# \n",
    "\n",
    "# Add a constant to the independent variable\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print out the statistics including t-values\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037039a1-c2e4-47d6-8cb0-163949a7c8d1",
   "metadata": {},
   "source": [
    "Based on the chart above, we see that the estimated model is $$\\hat y = 2.0163 + 0.3191X.$$ The chart also provides information about whether the coefficient and intercept are statistically significant. We see that the p-value for both is estimated to be $0.000$, which means that it is so small, it is rounded down to 0 when considering 3 decimal places. The $t$-scores are quite large, so this is not surprising. \n",
    "\n",
    "The $F$ statistic is also computed with respect to the *entire* model. Here, we are considering both the coefficient and the intercept. We see that our $F$-score is also large, which corresponds to a very small p-value ($3.00 \\times 10^{-27}$). \n",
    "\n",
    "We also get several other important metrics, including the coefficient of determination, $R^2$, which is estimated to be 0.698. This means that our model effectively captures 69.8% of the variation seen in $y$. \n",
    "\n",
    "Finally, it is important to highlight that with synthetic data, we know the true model and we can see that the estimated model, while close, is different. This is because all of our estimation practices will be effected by the noise expected in the data. The noisier the data, the larger the difference between the estimated model and the true model. This is also why smaller datasets are not ideal. The smaller the dataset, the more impacted it will be by the noise present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef1465-e611-4697-a1f9-fdfde390ae6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
