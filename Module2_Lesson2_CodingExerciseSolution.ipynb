{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539a100f-aa07-4bb5-ae78-6ce4f9d1a858",
   "metadata": {},
   "source": [
    "# Python Coding Exercise for Module 2, Lesson 2: Subset Selection\n",
    "\n",
    "Create a Python function that performs forward stepwise selection to identify a subset of predictor variables that are most related to a target variable in a linear regression context.\n",
    "\n",
    "### Your tasks are to write a function that satisfies the following:\n",
    "\n",
    "- Accept three parameters: X, a pandas DataFrame of predictor variables; y, a pandas Series of the target variable; and criterion, a string that specifies the metric used for variable selection (e.g., 'AIC', 'BIC', 'R-squared').\n",
    "- Start with an empty model and iteratively add the variable that improves the model the most based on the specified criterion until no variables improve the model.\n",
    "- Return a list of models with scores.\n",
    "\n",
    "### Constraints:\n",
    "\n",
    "- Use the statsmodels library to fit linear regression models.\n",
    "- Assume that X and y have been preprocessed and are ready for modeling (e.g., no missing values or categorical variables needing encoding).\n",
    "\n",
    "### Important Considerations:\n",
    "\n",
    "Although we have not discussed this yet, the smaller AIC and BIC scores are, the better. So you will need to adjust the previous code of forward stepwise selection to choose smaller values over larger ones. You are also provided with several functions that you can use to answer the questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992eb0e7-20d8-4841-8b71-7d719153925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7ad4f-a9aa-4a1d-a29c-cb57d1644a67",
   "metadata": {},
   "source": [
    "Below is a function you can use inside forward_stepwise_selection to score your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb8b91-fb54-4cea-9749-f412465b6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(criterion, model):\n",
    "    if criterion == 'AIC':\n",
    "        return model.aic \n",
    "    if criterion == 'BIC':\n",
    "        return model.bic \n",
    "    if criterion == 'R-squared':\n",
    "        return -model.rsquared # negative since higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f391a-4cc7-4013-b066-4bbc87eefa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(selected_features):\n",
    "        model = sm.OLS(y, sm.add_constant(X[selected_features])).fit()\n",
    "        return model\n",
    "\n",
    "def forward_stepwise_selection(data, response_variable, criterion='AIC'):\n",
    "    # Initialize variables\n",
    "    remaining_predictors = list(data.columns)\n",
    "    current_predictors = []\n",
    "    models = []\n",
    "\n",
    "    # Loop through the features\n",
    "    for i in range(data.shape[1]):\n",
    "        # Initialize best_candidate_score, best_candidate, and trial_columns\n",
    "        # Keep in mind that smaller scores are better. What should the initial score be?\n",
    "        best_candidate_score = float('inf')\n",
    "        best_candidate = None\n",
    "        trial_columns = list(current_predictors)  # Create a copy to avoid side effects\n",
    "\n",
    "        # for each predictor, consider the addition, the model, and the associated score\n",
    "        for predictor in remaining_predictors:\n",
    "            trial_columns.append(predictor)\n",
    "\n",
    "            model = fit_model(trial_columns)\n",
    "            criterion_value = score(criterion, model)\n",
    "\n",
    "            # if the score is better, update best_candidate and best_candidate_score\n",
    "            if criterion_value < best_candidate_score:\n",
    "                best_candidate_score = criterion_value\n",
    "                best_candidate = predictor\n",
    "            \n",
    "            trial_columns.remove(predictor)\n",
    "        \n",
    "        if best_candidate:\n",
    "            current_predictors.append(best_candidate)\n",
    "            remaining_predictors.remove(best_candidate)\n",
    "            models.append((list(current_predictors), best_candidate_score))\n",
    "\n",
    "    # return the list of models, sorted from low to high!\n",
    "    return sorted(models, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ca54c-078f-4f96-bd1b-8f2d984cad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset for demonstration (with 100 samples and 10 features)\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
    "X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(1, 11)])\n",
    "y = pd.Series(y, name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36beba-630e-40f8-b873-6bf37d4b3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the statistics on the full model tell us?\n",
    "\n",
    "full_model = fit_model([f'feature_{i}' for i in range(1, 11)])\n",
    "print(full_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08790a9-a031-4e70-9216-7a93dac52dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function call\n",
    "models = forward_stepwise_selection(X, y, criterion='R-squared')\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca525c4-1d94-49ed-b142-6f1e54da5305",
   "metadata": {},
   "source": [
    "Here, we can plot the list models by considering their number of features and their associated score. What does this graph suggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719d597-9514-4c8d-8098-5f8414b4857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_features_vs_score(data):\n",
    "    # Unpack the number of features and their associated scores\n",
    "    num_features = [len(features) for features, score in data]\n",
    "    scores = [score for features, score in data]\n",
    "    \n",
    "    # Create a plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(num_features, scores, marker='o')\n",
    "    \n",
    "    # Setting the axis labels\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    # Set title\n",
    "    plt.title('Number of Features vs. Score')\n",
    "    \n",
    "    # Show grid\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_features_vs_score(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63821ea9-ab78-423b-b30d-da2ab083b9e8",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "Based on your work above, answer the following questions. \n",
    "\n",
    "1. When using AIC to score, what is the recommended model? What is the score this model receives? Does the graph confirm this?\n",
    "2. When using BIC to score, what is the recommended model? What is the score this model receives? Does the graph confirm this?\n",
    "3. When using $R^2$ to score, we know every additional variable will improve the model. What was the score received for the full model? Based on the graph, what model would you pick? What score did it receive?\n",
    "4. Considering all of the results you've looked at, what model would you recommend using?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0b752-e7e7-4af5-a379-a9f7592b104d",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "\n",
    "1. When using the AIC criterion, all 10 features are recommended to model the changes in the dependent variable. The score the model received was -162.40399146442417, which is significantly lower than the next best score. The graph seems to confirm the choice of using all 10 features since the 10th feature results in a dramatic drop in the score.\n",
    "\n",
    "2. When using the BIC criterion, all 10 features are recommended to model the changes in the dependent variable. The score the model received was -133.74711941855514, which is significantly lower than the next best score. The graph seems to confirm the choice of using all 10 features since the 10th feature results in a dramatic drop in the score.\n",
    "\n",
    "3. When using the coefficient of determination, the full model receives a value of $R^2 = 0.999999746381322$, which is similar to the next best model. When we look at the graph, we would likely deduce that 6 features are sufficient for modeling the changes in the dependent variable, namely $\\beta_2, \\beta_4, \\beta_5, \\beta_6, \\beta_7$ and $\\beta_{10}$.\n",
    "\n",
    "4. Considering all the results above, it is likely best to consider the full model, especially because all coefficients are shown to be statistically significant. That said, special attention should be paid to features $X_2, X_4, X_5, X_6, X_7,$ and $X_{10}$, since they appear to model the majority of the change in $\\hat y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4bf11-57f3-4a67-a897-63749723bccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
